---
title: "Basic ML (TIDY)"
format: html
---

> DISCLAIMER: This content is copied from this [kaggle notebook](https://www.kaggle.com/code/camnugent/introduction-to-machine-learning-in-r-tutorial/notebook) authored by Cam Nugent


## Load the data

```{r}
#| message: false
#| warning: false

library(tidyverse)
library(kableExtra)

bs_style <- c("striped", "hover", "condensed", "responsive")
options(kable_styling_bootstrap_options = bs_style)
kbl <- function(data) kableExtra::kbl(data) %>% kable_styling()

housing <- read_csv(here::here("data", "housing.csv"))
kbl(head(housing))

```

We want to predict the median house value.

## Explore the data


```{r}
skimr::skim(housing)
count(housing, ocean_proximity)
```


```{r}
housing %>% 
  pivot_longer(cols = -ocean_proximity, names_to = "variable",
               values_to = "value") %>% 
  ggplot(aes(value)) +
  geom_histogram(bins = 30) +
  facet_wrap(~variable, scales = "free_x")
```

## Data cleaning

1. missing value imputation for total bedrooms
2. make the total cols as mean
3. Scale the numerical values
4. Turn categories into booleans

```{r}
housing %>% 
  mutate(
    # cleaning step 01
    total_bedrooms = replace_na(total_bedrooms, 
                                replace = median(.$total_bedrooms, na.rm = TRUE)),
    # cleaning step 02
    mean_bedrooms = total_bedrooms / households,
    mean_rooms = total_rooms / households
  ) %>% 
  # dropping the total columns
  select(-c(total_bedrooms, total_rooms)) %>% 
  mutate(
    # cleaning step 03
    across(.cols = -c(ocean_proximity, median_house_value), .fns = scale),
    value = 1 # for one-hot-encoding
  ) %>% 
  pivot_wider(
    names_from = ocean_proximity, values_from = value, values_fill = 0
  ) -> cleaned_housing

kbl(head(cleaned_housing))

skimr::skim(cleaned_housing)
```



## Create Train Test data

```{r}
set.seed(1738)

train_sample <- sample.int(n = nrow(cleaned_housing), 
                           size = floor(0.8 * nrow(cleaned_housing)),
                           replace = FALSE)

train <- cleaned_housing[train_sample, ]
test <- cleaned_housing[-train_sample, ]

kbl(head(train))

nrow(train) + nrow(test) == nrow(cleaned_housing)
```

## Modelling and K-fold CV

```{r}
library(boot)

glm_house <- glm(median_house_value ~ median_income + mean_rooms + population, 
                 data = cleaned_housing)
glm_house
```

```{r}
k_fold_cv_error = cv.glm(cleaned_housing, glm_house, K = 5)

k_fold_cv_error$delta
```

```{r}
glm_cv_rmse = sqrt(k_fold_cv_error$delta[1])
glm_cv_rmse
```

## Random Forest model

```{r}
library(randomForest)
```

```{r}
set.seed(1738)

train_y = train[, 'median_house_value', drop=TRUE]
train_x = train[, names(train) != 'median_house_value']

kbl(head(train_x))
kbl(head(train_y))
```

```{r}
rf_model = randomForest(train_x, y = train_y, ntree = 500, importance = TRUE)

oob_prediction <- predict(rf_model) # leaving out the data source forces on predictions

train_mse <- mean(as.numeric(oob_prediction - train_y)^2)
oob_rmse <- sqrt(train_mse)
oob_rmse
```
And what about the `rmse` in the test data??

```{r}
test_y <- test[, 'median_house_value', drop = TRUE]
test_x <- test[, names(test) != 'median_house_value']

y_pred <- predict(rf_model, test_x)
test_mse <- mean((y_pred - test_y)^2)
test_rmse <- sqrt(test_mse)
test_rmse
```

