---
title: "Basic ML 02"
format: html
execute: 
  warning: false
---

> DISCLAIMER: all (mostly!) the codes are copied from [this place](https://machinelearningmastery.com/machine-learning-in-r-step-by-step/)

## Setup

```{r}
#| message: false

library(caret)
library(tidyverse)

# partition the data
set.seed(11)
train_idx <- createDataPartition(iris$Species, p = 0.8, list = FALSE)
dataset <- iris[train_idx, ]
validation <- iris[-train_idx, ]
```

## Data exploration

```{r}
# dimension of the dataset
dim(dataset)

# class attributes of columns
sapply(dataset, class)

# peek at the data
head(dataset)

# levels of the class
levels(dataset$Species)

# class distribution
percentage <- prop.table(table(dataset$Species)) * 100
cbind(freq = table(dataset$Species), percentage = percentage)

# summary
summary(dataset)
```

### vizulizing the data

```{r}
dataset %>% 
  pivot_longer(cols = -Species, names_to = "variable", values_to = "value") %>% 
  ggplot(aes(x = variable, y = value)) +
  geom_boxplot() + theme_bw()
```


```{r}
x <- dataset[, 1:4]
y <- dataset[, 5]

# featurePlot(x = x, y = y, plot = "ellipse") # does not work

library(ggforce)

ellipse_plot <- function(data, x, y, group) {
  ggplot(data, aes({{ x }}, {{ y }}, fill = {{ group }})) +
    ggforce::geom_mark_ellipse() +
    geom_point()
}

ellipse_plot(dataset, Sepal.Length, Sepal.Width, Species)
ellipse_plot(dataset, Sepal.Length, Petal.Length, Species)

ellipse_plot_str <- function(data, x, y, group) {
  ggplot(data, aes(x = .data[[x]], y = .data[[y]], fill = .data[[group]])) +
    ggforce::geom_mark_ellipse() +
    geom_point() +
    theme_bw()
}

attr <- names(dataset[, 1:3])
attr <- set_names(attr)

ellipse_plots <- map(attr, ~ellipse_plot_str(dataset, .x, "Petal.Width", "Species"))
walk(ellipse_plots, print)
```

```{r}
featurePlot(x=x, y=y, plot="box")

dataset %>% 
  pivot_longer(cols = -Species, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(Species, values)) +
  geom_boxplot() +
  facet_wrap(~variable, scales = "free_y") +
  theme_bw()
```

```{r}
# density plots for each attribute by class value
scales <- list(x=list(relation="free"), y=list(relation="free"))
featurePlot(x=x, y=y, plot="density", scales=scales)

dataset %>% 
  pivot_longer(cols = -Species, names_to = "variable", values_to = "values") %>% 
  ggplot(aes(values, color = Species)) +
  geom_density() +
  geom_rug() +
  facet_wrap(~variable, scales = "free") +
  theme_bw()
```

## Cross Validation


```{r}
control <- trainControl(method = "cv", number = 10)
metric <- "Accuracy"
```


## Build models

We will consider,

- LDA (linear method)
- CART, knn (simple nonlinear)
- SVM (with linear kernel), RF (complex nonlinear)

```{r}
# 1. linear algorithms
# lda
set.seed(11)
fit_lda <- train(Species ~ ., data = dataset, method = "lda", 
                 metric = metric, trControl = control)

# 2. nonlinear algorithms
# CART
set.seed(11)
fit_cart <- train(Species ~ ., data = dataset, method = "rpart", 
                  metric = metric, trControl = control)

# knn
set.seed(11)
fit_knn <- train(Species ~ ., data = dataset, method = "knn", 
                  metric = metric, trControl = control)


# advanced
# SVM
set.seed(11)
fit_svm <- train(Species ~ ., data = dataset, method = "svmRadial", 
                  metric = metric, trControl = control)

# Random Forest
set.seed(11)
fit_rf <- train(Species ~ ., data = dataset, method = "rf", 
                  metric = metric, trControl = control)

```


```{r}
results <- resamples(list(lda = fit_lda, cart = fit_cart, knn = fit_knn,
                          svm = fit_svm, rf = fit_rf))
summary(results)
```

```{r}
dotplot(results)
```

**`lda`is more accurate for this case**

```{r}
print(fit_lda)
```

## Make predictions

```{r}
predictions <- predict(fit_lda, validation)

confusionMatrix(predictions, validation$Species)
```

